from fastapi import FastAPI, UploadFile, File
from PIL import Image
import io
import traceback
import torch

from app.model import get_model
from app.utils import preprocess_image
from app.face_utils import detect_and_crop_face
from app.quality_check import quality_check
from app.decision import decide

app = FastAPI()

# =========================
# GLOBAL MODEL
# =========================
model = None

# =========================
# CLASS CONFIG (‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏á‡∏ï‡∏≠‡∏ô train)
# =========================
BMI_LABELS = {
    0: "under",
    1: "normal",
    2: "over"
}

BMI_STATUS_TH = {
    "under": "‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå",
    "normal": "‡∏™‡∏°‡∏™‡πà‡∏ß‡∏ô",
    "over": "‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå"
}

# =========================
# HEALTH CHECK
# =========================
@app.get("/")
def health():
    return {
        "status": "ok",
        "service": "BMI AI Backend"
    }

# =========================
# LOAD MODEL
# =========================
@app.on_event("startup")
def startup_event():
    global model
    print("üöÄ Loading model...")
    model = get_model()
    model.eval()
    print("‚úÖ Model ready")

# =========================
# PREDICT
# =========================
@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    try:
        # 1Ô∏è‚É£ Read image
        image_bytes = await file.read()
        if not image_bytes:
            return {
                "error": "empty_file",
                "message": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û"
            }

        image = Image.open(io.BytesIO(image_bytes)).convert("RGB")

        # 2Ô∏è‚É£ Quality check (soft check)
        ok, reason = quality_check(image)
        if not ok:
            print(f"‚ö†Ô∏è Quality warning: {reason}")

        # 3Ô∏è‚É£ Detect face (‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö ‡πÅ‡∏ï‡πà‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à)
        face_image, has_face = detect_and_crop_face(image)

        # 4Ô∏è‚É£ Preprocess (224x224)
        x = preprocess_image(face_image)
        x = x.to(next(model.parameters()).device)

        # 5Ô∏è‚É£ Predict
        with torch.no_grad():
            logits = model(x)
            probs = torch.softmax(logits, dim=1)

            cls_idx = int(probs.argmax(dim=1).item())
            cls_name = BMI_LABELS[cls_idx]
            confidence = float(probs[0, cls_idx])

        # 6Ô∏è‚É£ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏´‡∏ô‡πâ‡∏≤ ‚Üí ‡∏•‡∏î confidence
        if not has_face:
            confidence *= 0.7

        # 7Ô∏è‚É£ Decision layer (‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏à‡∏£‡∏¥‡∏á)
        decision = decide(cls_name, confidence)

        if not decision["ok"]:
            return {
                "error": "low_confidence",
                "message": decision["message"]
            }

        # 8Ô∏è‚É£ Final response
        return {
            "status": BMI_STATUS_TH[decision["class"]],
            "confidence": decision["confidence"]
        }

    except Exception:
        traceback.print_exc()
        return {
            "error": "prediction_failed",
            "message": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô BMI ‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ"
        }
